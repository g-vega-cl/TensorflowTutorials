{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2020_01_15-LoadTextTFTutoria.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN91iErcoaNMMQnLHsUZ7VM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"wkXMh-NsHhdw","colab_type":"code","colab":{}},"source":["#https://www.tensorflow.org/tutorials/load_data/text"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6uCjBAkzjWa0","colab_type":"text"},"source":["This tutorial provides an example of how to use `tf.data.TextLineDataset` to load examples from text files. TextLineDataset is designed to create a dataset from a text file, in which each example is a line of text from the original file. This is potentially useful for any text data that is primarily line-based (for example, poetry or error logs).\n","\n","In this tutorial, we'll use three different English translations of the same work, Homer's Illiad, and train a model to identify the translator given a single line of text."]},{"cell_type":"code","metadata":{"id":"fNqvpJFPHpgh","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","# TensorFlow and tf.keras\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"63TZs0mIjby6","colab_type":"code","colab":{}},"source":["import os\n","import tensorflow_datasets as tfds"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"POp7dtr4jh-3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"b637b466-8adb-4c3b-ce00-db6ff296cc5e","executionInfo":{"status":"ok","timestamp":1579146693395,"user_tz":360,"elapsed":681,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["#Getting the data from google\n","DIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n","FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\n","\n","for name in FILE_NAMES:\n","  text_dir = tf.keras.utils.get_file(name, origin=DIRECTORY_URL+name)\n","  \n","parent_dir = os.path.dirname(text_dir)\n","\n","parent_dir"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/root/.keras/datasets'"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"LwoX1uMEjxfK","colab_type":"text"},"source":["#Load text into datasets\n","\n","Iterate through the files, loading each one into its own dataset.\n","\n","Each example needs to be individually labeled, so use tf.data.Dataset.map to apply a labeler function to each one. This will iterate over every example in the dataset, returning (example, label) pairs."]},{"cell_type":"code","metadata":{"id":"NZkIVE1Yjo4X","colab_type":"code","colab":{}},"source":["def labeler(example, index):\n","  return example, tf.cast(index, tf.int64) #remember .cast changes the type of data \n","\n","labeled_data_sets = []\n","\n","for i, file_name in enumerate(FILE_NAMES):\n","  lines_dataset = tf.data.TextLineDataset(os.path.join(parent_dir, file_name)) #A Dataset comprising lines from one or more text files\n","  labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i)) #Creo que a TextLineDataset ya le tienes que pasar el texto como lineas\n","  labeled_data_sets.append(labeled_dataset)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xw2k1hf2j8ca","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"5cd66f4d-df8f-49bc-8ef3-1683eb33c2d5","executionInfo":{"status":"ok","timestamp":1579146693399,"user_tz":360,"elapsed":668,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["lines_dataset #An array of type: <TextLineDatasetV2 shapes: (), types: tf.string>\n","labeled_dataset #A mapDataset <MapDataset shapes: ((), ()), types: (tf.string, tf.int64)>\n","labeled_data_sets #Its MapDataset, but 3 of them. (because we have 3 files)"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<MapDataset shapes: ((), ()), types: (tf.string, tf.int64)>,\n"," <MapDataset shapes: ((), ()), types: (tf.string, tf.int64)>,\n"," <MapDataset shapes: ((), ()), types: (tf.string, tf.int64)>]"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"pzC2BtF6lyCN","colab_type":"code","colab":{}},"source":["for i, file_name in enumerate(FILE_NAMES):\n","  file_name"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rzAeNVw4kA-5","colab_type":"code","colab":{}},"source":["BUFFER_SIZE = 50000\n","BATCH_SIZE = 64\n","TAKE_SIZE = 5000\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gynSY0tUkn5P","colab_type":"code","colab":{}},"source":["#Combine these labeled datasets into a single dataset, and shuffle it.\n","all_labeled_data = labeled_data_sets[0]\n","for labeled_dataset in labeled_data_sets[1:]:\n","  all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n","  \n","all_labeled_data = all_labeled_data.shuffle(\n","    BUFFER_SIZE, reshuffle_each_iteration=False) #Buffer size: representing the number of elements from this dataset from which the new dataset will sample.\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X4Dz0jtTkvyG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":129},"outputId":"1f01732a-bc22-40cb-c1a1-80f06f9b88f5","executionInfo":{"status":"ok","timestamp":1579146694687,"user_tz":360,"elapsed":1928,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["#You can use tf.data.Dataset.take and print to see what the (example, label) pairs look like. \n","#The numpy property shows each Tensor's value.\n","for ex in all_labeled_data.take(5):\n","  print(ex)\n","#I think until now we make each line a \"observation\"\n"],"execution_count":27,"outputs":[{"output_type":"stream","text":["(<tf.Tensor: shape=(), dtype=string, numpy=b'And the wind filled it. Roared the sable flood'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n","(<tf.Tensor: shape=(), dtype=string, numpy=b'knees of Minerva, lay the largest and fairest robe you have in your'>, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n","(<tf.Tensor: shape=(), dtype=string, numpy=b'Lived not his lusty manhood to enjoy,'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n","(<tf.Tensor: shape=(), dtype=string, numpy=b'Fell Stichius and Arcesilas; the one,'>, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n","(<tf.Tensor: shape=(), dtype=string, numpy=b'A heron, by command of Pallas, flew'>, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2lAkPfXVmiUZ","colab_type":"text"},"source":["#Encode text lines as numbers\n","\n","Machine learning models work on numbers, not words, so the string values need to be converted into lists of numbers. To do that, map each unique word to a unique integer.\n","Build vocabulary\n","\n","First, build a vocabulary by tokenizing the text into a collection of individual unique words. There are a few ways to do this in both TensorFlow and Python. For this tutorial:\n","\n","    Iterate over each example's numpy value.\n","    Use tfds.features.text.Tokenizer to split it into tokens.\n","    Collect these tokens into a Python set, to remove duplicates.\n","    Get the size of the vocabulary for later use.\n"]},{"cell_type":"code","metadata":{"id":"WAANzCw4lQSK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"2720af1d-70f0-4777-c808-2cd3112d20cd","executionInfo":{"status":"ok","timestamp":1579146699403,"user_tz":360,"elapsed":6634,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["tokenizer = tfds.features.text.Tokenizer()\n","\n","vocabulary_set = set()\n","for text_tensor, _ in all_labeled_data: #all_labeled_data contains all lines, _ is the label, the label is to which text it belongs\n","  some_tokens = tokenizer.tokenize(text_tensor.numpy())\n","  vocabulary_set.update(some_tokens)\n","\n","vocab_size = len(vocabulary_set)\n","vocab_size"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["17178"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"mGsgGUjbnlGI","colab_type":"text"},"source":["#Okay, so to tokenize text:\n","\n","\n","```\n","is as simple as:\n","stringToTokenize = stringToTokenize = \"a big horse went over my house and it died hard, now i added a comma\"\n","tokens = tokenizer.tokenize(stringToTokenize)\n","tokens # <- has the tokenized dic\n","```\n","then to remove repeated words:\n","\n","\n","```\n","vocabulary_set = set()\n","vocabulary_set.update(tokens)\n","vocabulary_set\n","```\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"I44eJBUJoZ8r","colab_type":"text"},"source":["#Encode examples\n","\n","Create an encoder by passing the vocabulary_set to tfds.features.text.TokenTextEncoder. The encoder's encode method takes in a string of text and returns a list of integers."]},{"cell_type":"code","metadata":{"id":"gRU6tuxkoZxb","colab_type":"code","colab":{}},"source":["encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"4682571f-6e14-48b0-9e63-cc8152746e2a","executionInfo":{"status":"ok","timestamp":1579146699410,"user_tz":360,"elapsed":6622,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}},"id":"4sUvMeuxnIYq","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["encoder #It is a tensorflow type TokenTextEncoder"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<TokenTextEncoder vocab_size=17180>"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"s_0j9dPtntVi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"0c999a19-6ea1-4d09-8124-a25254283f33","executionInfo":{"status":"ok","timestamp":1579146700450,"user_tz":360,"elapsed":7642,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["example_text = next(iter(all_labeled_data))[0].numpy()\n","print(example_text)"],"execution_count":31,"outputs":[{"output_type":"stream","text":["b'And the wind filled it. Roared the sable flood'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rKJTJCe3Kvmt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"643322e5-d01f-4f12-d450-f4a185bb6c97","executionInfo":{"status":"ok","timestamp":1579146700452,"user_tz":360,"elapsed":7619,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["encoded_example = encoder.encode(example_text)\n","print(encoded_example) #Literal pasaron los tokens a número. "],"execution_count":32,"outputs":[{"output_type":"stream","text":["[8877, 7411, 6862, 16693, 3627, 10645, 7411, 5476, 8013]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9Nlg32ovLOvG","colab_type":"code","colab":{}},"source":["def encode(text_tensor, label): #text_tensor es la string\n","  encoded_text = encoder.encode(text_tensor.numpy())\n","  return encoded_text, label\n","\n","def encode_map_fn(text, label):\n","  return tf.py_function(encode, inp=[text, label], Tout=(tf.int64, tf.int64)) #https://www.tensorflow.org/api_docs/python/tf/py_function\n","  #I think\n","\n","all_encoded_data = all_labeled_data.map(encode_map_fn)#pass that function to \n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BOJ1PbwEi6f8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":74},"outputId":"ae78fefe-ec00-4bdf-d072-f7f3749971c0","executionInfo":{"status":"ok","timestamp":1579148243155,"user_tz":360,"elapsed":1258,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["label_and_text = next(iter(all_labeled_data))\n","label_and_text[1]\n","\n","encodedmapfn = encode_map_fn(label_and_text[0],label_and_text[1])\n","encodedmapfn #Basicamente regresa el texto como integers y la label, nota: encodedmapfn[0] es el texto y [1] es la label"],"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<tf.Tensor: shape=(9,), dtype=int64, numpy=array([ 8877,  7411,  6862, 16693,  3627, 10645,  7411,  5476,  8013])>,\n"," <tf.Tensor: shape=(), dtype=int64, numpy=0>]"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"markdown","metadata":{"id":"k28PSKXTkn5U","colab_type":"text"},"source":["#Split the dataset into test and train batches\n","\n","Use `tf.data.Dataset.take` and `tf.data.Dataset.skip` to create a small test dataset and a larger training set.\n","\n","Before being passed into the model, the datasets need to be batched. Typically, the examples inside of a batch need to be the same size and shape. But, the examples in these datasets are not all the same size — each line of text had a different number of words. So use `tf.data.Dataset.padded_batch` (instead of batch) to pad the examples to the same size."]},{"cell_type":"code","metadata":{"id":"MlA6XPaMketd","colab_type":"code","colab":{}},"source":["train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE) #The take is to randomly take n ammount of 'rows'\n","train_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]))\n","\n","test_data = all_encoded_data.take(TAKE_SIZE) #Is the train data and test data the same size\n","test_data = test_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2HbEVBXymLZj","colab_type":"text"},"source":["Now, test_data and train_data are not collections of (example, label) pairs, but collections of batches. Each batch is a pair of (many examples, many labels) represented as arrays."]},{"cell_type":"code","metadata":{"id":"Y4VjoLSjlnx3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":91},"outputId":"41faaad5-afe1-4034-df86-63c18d322647","executionInfo":{"status":"ok","timestamp":1579148849836,"user_tz":360,"elapsed":1577,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["sample_text, sample_labels = next(iter(test_data))\n","\n","sample_text[63], sample_labels[0] #all sample_text has the same shape. same with labels.\n","#If we place the index too high it gives an error because its outside the batch\n","#Batch_size is 64, therefore 63 is the last achievable index"],"execution_count":67,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(15,), dtype=int64, numpy=\n"," array([ 4946,  5877, 15770,  5877, 12744, 13297,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0])>,\n"," <tf.Tensor: shape=(), dtype=int64, numpy=0>)"]},"metadata":{"tags":[]},"execution_count":67}]},{"cell_type":"code","metadata":{"id":"DTkNK45xmPl4","colab_type":"code","colab":{}},"source":["vocab_size += 1 #Because the 0 is added to the encoding (the dictionary)\n","#Note, dont really use this cell more than once because it will increase your dic size uneccesarly"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9qjGItlnnIxZ","colab_type":"text"},"source":["#Build the model"]},{"cell_type":"code","metadata":{"id":"Ch-RXWoim-RW","colab_type":"code","colab":{}},"source":["model = tf.keras.Sequential()\n","#The first layer converts integer representations to dense vector embeddings. See the word embeddings tutorial or more details.\n","model.add(tf.keras.layers.Embedding(vocab_size, 64))\n","#The next layer is a Long Short-Term Memory layer, \n","#which lets the model understand words in their context with other words. \n","#A bidirectional wrapper on the LSTM helps it to learn about the datapoints in relationship \n","#to the datapoints that came before it and after it.\n","model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n","#Finally we'll have a series of one or more densely connected layers, with the last one being the output layer. \n","#The output layer produces a probability for all the labels. \n","#The one with the highest probability is the models prediction of an example's label.\n","\"\"\"\n","inally we'll have a series of one or more densely connected layers, with the last one being the output layer. \n","The output layer produces a probability for all the labels. \n","The one with the highest probability is the models prediction of an example's label.\n","\"\"\"\n","for units in [64, 64]:# Edit the list in the `for` line to experiment with layer sizes. Note, this just gives the value 64 twice\n","  model.add(tf.keras.layers.Dense(units, activation='relu'))\n","\n","# Output layer. The first argument is the number of labels. because we get probabilities for each label\n","model.add(tf.keras.layers.Dense(3, activation='softmax'))\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8uALiw5eob8X","colab_type":"text"},"source":["Finally, compile the model. For a softmax categorization model, use sparse_categorical_crossentropy as the loss function. You can try other optimizers, but adam is very common."]},{"cell_type":"code","metadata":{"id":"yF03q-u9n_vl","colab_type":"code","colab":{}},"source":["model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9TwBW3IWo1M1","colab_type":"text"},"source":["#Train the model"]},{"cell_type":"code","metadata":{"id":"nwmL0p8jo0dW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":166},"outputId":"aa0c595f-fb98-4abc-e254-8d190128c3d5","executionInfo":{"status":"ok","timestamp":1579149492315,"user_tz":360,"elapsed":93557,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["#This model running on this data produces decent results (about 83%).\n","model.fit(train_data, epochs=3, validation_data=test_data)\n"],"execution_count":72,"outputs":[{"output_type":"stream","text":["Epoch 1/3\n","697/697 [==============================] - 39s 56ms/step - loss: 0.5312 - accuracy: 0.7448 - val_loss: 0.3974 - val_accuracy: 0.8252\n","Epoch 2/3\n","697/697 [==============================] - 27s 39ms/step - loss: 0.3052 - accuracy: 0.8665 - val_loss: 0.3893 - val_accuracy: 0.8290\n","Epoch 3/3\n","697/697 [==============================] - 27s 38ms/step - loss: 0.2330 - accuracy: 0.8986 - val_loss: 0.4096 - val_accuracy: 0.8284\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f8f09471f98>"]},"metadata":{"tags":[]},"execution_count":72}]},{"cell_type":"code","metadata":{"id":"PjKLywW6o5IZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"2af50f60-53c3-4026-be21-7dda933f5c47","executionInfo":{"status":"ok","timestamp":1579149537240,"user_tz":360,"elapsed":3126,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["eval_loss, eval_acc = model.evaluate(test_data)\n","print('\\nEval loss: {:.3f}, Eval accuracy: {:.3f}'.format(eval_loss, eval_acc))\n"],"execution_count":73,"outputs":[{"output_type":"stream","text":["     79/Unknown - 3s 37ms/step - loss: 0.4096 - accuracy: 0.8284\n","Eval loss: 0.410, Eval accuracy: 0.828\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bQFVjMNmpaLy","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}