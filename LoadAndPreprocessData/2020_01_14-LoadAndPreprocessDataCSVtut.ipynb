{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2020_01_14-LoadAndPreprocessDataCSVtut.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMl1LdYPAaxzkOWPd2Ot1O0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"Zi5UeBZTLoK7","colab_type":"code","colab":{}},"source":["#https://www.tensorflow.org/tutorials/load_data/csv\n","#This tutorial provides an example of how to load CSV data from a file into a tf.data.Dataset."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qQOZuILwLp_J","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","# TensorFlow and tf.keras\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NAatQyh8L2b0","colab_type":"code","colab":{}},"source":["import functools"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O5dKw4uvL5Bf","colab_type":"code","colab":{}},"source":["#we have not download any data, its just getting the links and paths to data.\n","#but it seems we have, the folder is just hidden?\n","#Note: the csv is as any csv, columns and rows with data \n","TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n","TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n","\n","train_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\n","test_file_path = tf.keras.utils.get_file(\"eval.csv\", TEST_DATA_URL)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CgsDXvMZMF1S","colab_type":"code","colab":{}},"source":["# Make numpy values easier to read.\n","np.set_printoptions(precision=3, suppress=True) #Just setting some options\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XlJcSVWf2hcn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":202},"outputId":"e045cba6-abd2-4240-85c2-7226c184e1a4","executionInfo":{"status":"ok","timestamp":1579060874075,"user_tz":360,"elapsed":2871,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["!head {train_file_path} #checking the head of the data\n","#My theory is that this commands take the data from the url directly\n","\n"],"execution_count":42,"outputs":[{"output_type":"stream","text":["survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone\n","0,male,22.0,1,0,7.25,Third,unknown,Southampton,n\n","1,female,38.0,1,0,71.2833,First,C,Cherbourg,n\n","1,female,26.0,0,0,7.925,Third,unknown,Southampton,y\n","1,female,35.0,1,0,53.1,First,C,Southampton,n\n","0,male,28.0,0,0,8.4583,Third,unknown,Queenstown,y\n","0,male,2.0,3,1,21.075,Third,unknown,Southampton,n\n","1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n\n","1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n\n","1,female,4.0,1,1,16.7,Third,G,Southampton,n\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5cviSLif28OE","colab_type":"text"},"source":["You can load this using pandas, and pass the NumPy arrays to TensorFlow. If you need to scale up to a large set of files, or need a loader that integrates with TensorFlow and tf.data then use the tf.data.experimental.make_csv_dataset function:![alt text](https://)"]},{"cell_type":"code","metadata":{"id":"p5jUbX362k8V","colab_type":"code","colab":{}},"source":["#The only column you need to identify explicitly is the one with the value \n","#that the model is intended to predict. \n","LABEL_COLUMN = 'survived'\n","LABELS = [0, 1]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4cwZuuKG3FuT","colab_type":"code","colab":{}},"source":["#Now read the CSV data from the file and create a dataset. \n","def get_dataset(file_path, **kwargs):\n","  dataset = tf.data.experimental.make_csv_dataset(\n","      file_path,\n","      batch_size=5, # Artificially small to make examples easier to show.\n","      label_name=LABEL_COLUMN,\n","      na_value=\"?\",\n","      num_epochs=1,\n","      ignore_errors=True, \n","      **kwargs)\n","  return dataset\n","\n","raw_train_data = get_dataset(train_file_path)\n","raw_test_data = get_dataset(test_file_path)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f_XhRXd-3OPq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"ff827cd9-6ef4-4497-c778-5fb63c966597","executionInfo":{"status":"ok","timestamp":1579060874204,"user_tz":360,"elapsed":2977,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["#interesting, this creates a PrefetchDataset from the csv, \n","raw_train_data"],"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<PrefetchDataset shapes: (OrderedDict([(sex, (None,)), (age, (None,)), (n_siblings_spouses, (None,)), (parch, (None,)), (fare, (None,)), (class, (None,)), (deck, (None,)), (embark_town, (None,)), (alone, (None,))]), (None,)), types: (OrderedDict([(sex, tf.string), (age, tf.float32), (n_siblings_spouses, tf.int32), (parch, tf.int32), (fare, tf.float32), (class, tf.string), (deck, tf.string), (embark_town, tf.string), (alone, tf.string)]), tf.int32)>"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"id":"iyUsl_RT3azW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":183},"outputId":"77447ee8-93f0-4065-8ccf-4e84daa846b0","executionInfo":{"status":"ok","timestamp":1579060874207,"user_tz":360,"elapsed":2971,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["\"\"\"\n","Each item in the dataset is a batch, represented as a tuple of (many examples, many labels). \n","The data from the examples is organized in column-based tensors (rather than row-based tensors), \n","each with as many elements as the batch size (5 in this case).\n","\"\"\"\n","def show_batch(dataset):\n","  for batch, label in dataset.take(1):\n","    for key, value in batch.items():\n","      print(\"{:20s}: {}\".format(key,value.numpy()))\n","\n","show_batch(raw_train_data) #Basically every 'column' has 5 rows of data.\n"],"execution_count":46,"outputs":[{"output_type":"stream","text":["sex                 : [b'female' b'female' b'male' b'male' b'female']\n","age                 : [28.  42.  22.  19.  14.5]\n","n_siblings_spouses  : [0 1 0 0 1]\n","parch               : [2 0 0 0 0]\n","fare                : [22.358 26.     8.05   7.65  14.454]\n","class               : [b'Third' b'Second' b'Third' b'Third' b'Third']\n","deck                : [b'unknown' b'unknown' b'unknown' b'F' b'unknown']\n","embark_town         : [b'Cherbourg' b'Southampton' b'Southampton' b'Southampton' b'Cherbourg']\n","alone               : [b'n' b'n' b'y' b'y' b'n']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kbmXwQzW4BYi","colab_type":"text"},"source":["NOTE:\n","the columns in the CSV are named. The dataset constructor will pick these names up automatically. If the file you are working with does not contain the column names in the first line, pass them in a list of strings to the column_names argument in the make_csv_dataset function."]},{"cell_type":"code","metadata":{"id":"peWz_nTY3dla","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":183},"outputId":"8f2d59e7-2092-42ad-e8f3-c4f907a0fdae","executionInfo":{"status":"ok","timestamp":1579060874209,"user_tz":360,"elapsed":2963,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["CSV_COLUMNS = ['survived', 'sex', 'age', 'n_siblings_spouses', 'parch', 'fare', 'class', 'deck', 'embark_town', 'alone']\n","temp_dataset = get_dataset(train_file_path, column_names=CSV_COLUMNS)\n","show_batch(temp_dataset)\n"],"execution_count":47,"outputs":[{"output_type":"stream","text":["sex                 : [b'female' b'female' b'male' b'male' b'female']\n","age                 : [19.  58.  70.5  4.  15. ]\n","n_siblings_spouses  : [0 0 0 3 0]\n","parch               : [0 1 0 2 0]\n","fare                : [ 30.    153.462   7.75   27.9     7.225]\n","class               : [b'First' b'First' b'Third' b'Third' b'Third']\n","deck                : [b'B' b'C' b'unknown' b'unknown' b'unknown']\n","embark_town         : [b'Southampton' b'Southampton' b'Queenstown' b'Southampton' b'Cherbourg']\n","alone               : [b'y' b'n' b'y' b'n' b'y']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"L8zAt1SN4RD9","colab_type":"text"},"source":["#Data preprocessing\n","A CSV file can contain a variety of data types. Typically you want to convert from those mixed types to a fixed length vector before feeding the data into your model.\n","\n","TensorFlow has a built-in system for describing common input conversions: tf.feature_column (see https://www.tensorflow.org/tutorials/structured_data/feature_columns for more info [another tut, we will get there] )\n","\n","You can preprocess your data using any tool you like (like nltk or sklearn), and just pass the processed output to TensorFlow.\n","\n","The primary advantage of doing the preprocessing inside your model is that when you export the model it includes the preprocessing. This way you can pass the raw data directly to your model."]},{"cell_type":"markdown","metadata":{"id":"fNwMpGaC4sQe","colab_type":"text"},"source":["**Continuous data**\n","\n","(If you have mixed datatypes you may want to separate out these simple-numeric fields. The tf.feature_column api can handle them, but this incurs some overhead and should be avoided unless really necessary)\n","\n","If your data is already in an appropriate numeric format, you can pack the data into a vector before passing it off to the model.\n","\n","(Basically everything that is not a string in this case)"]},{"cell_type":"code","metadata":{"id":"0eYhqqE_4FJT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":91},"outputId":"e38c2799-8c4d-464d-be3f-a0484417350e","executionInfo":{"status":"ok","timestamp":1579060874211,"user_tz":360,"elapsed":2954,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["SELECT_COLUMNS = ['survived', 'age', 'n_siblings_spouses', 'parch', 'fare']\n","DEFAULTS = [0, 0.0, 0.0, 0.0, 0.0]\n","temp_dataset = get_dataset(train_file_path, \n","                           select_columns=SELECT_COLUMNS,\n","                           column_defaults = DEFAULTS)\n","\n","show_batch(temp_dataset)\n"],"execution_count":48,"outputs":[{"output_type":"stream","text":["age                 : [31. 16. 28. 31. 28.]\n","n_siblings_spouses  : [1. 0. 0. 1. 0.]\n","parch               : [1. 0. 0. 1. 0.]\n","fare                : [37.004  7.733  0.    20.525  0.   ]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"k-oJReKZ5CMc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":185},"outputId":"6d63befb-048c-4456-afb1-07109527ac77","executionInfo":{"status":"ok","timestamp":1579060874458,"user_tz":360,"elapsed":3190,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["example_batch, labels_batch = next(iter(temp_dataset)) #take the next 5 values (the next batch)\n","example_batch #Note, show batch does not work with this one\n"],"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OrderedDict([('age',\n","              <tf.Tensor: shape=(5,), dtype=float32, numpy=array([43., 34., 27., 21.,  7.], dtype=float32)>),\n","             ('n_siblings_spouses',\n","              <tf.Tensor: shape=(5,), dtype=float32, numpy=array([1., 0., 0., 0., 4.], dtype=float32)>),\n","             ('parch',\n","              <tf.Tensor: shape=(5,), dtype=float32, numpy=array([1., 1., 0., 0., 1.], dtype=float32)>),\n","             ('fare',\n","              <tf.Tensor: shape=(5,), dtype=float32, numpy=array([26.25 , 23.   ,  7.896, 77.958, 29.125], dtype=float32)>)])"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"code","metadata":{"id":"BGX6GbAR5zYV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":146},"outputId":"8541b3cd-7e19-4382-dc35-a2026cac413b","executionInfo":{"status":"ok","timestamp":1579060874461,"user_tz":360,"elapsed":3183,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["#Here's a simple function that will pack together all the columns:\n","def pack(features, label):\n","  return tf.stack(list(features.values()), axis=-1), label\n","\n","#Apply this to each element of the dataset:\n","packed_dataset = temp_dataset.map(pack)\n","\n","for features, labels in packed_dataset.take(1):\n","  print(features.numpy())\n","  print()\n","  print(labels.numpy())\n","\n","#returns rows with the values (age, n_siblings...)\n","#And its label"],"execution_count":50,"outputs":[{"output_type":"stream","text":["[[25.     0.     0.     7.65 ]\n"," [34.     1.     1.    32.5  ]\n"," [26.     2.     0.     8.663]\n"," [32.     0.     0.    30.5  ]\n"," [38.     0.     0.    80.   ]]\n","\n","[0 1 0 1 1]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cAa5u04a6YoY","colab_type":"code","colab":{}},"source":["#Since our data is mixed (numerical and non-numerical)\n","# we define a preprocessor that selects a list of numeric features and packs them into a single column:\n","class PackNumericFeatures(object):\n","  def __init__(self, names):\n","    self.names = names\n","\n","  def __call__(self, features, labels):\n","    numeric_features = [features.pop(name) for name in self.names] #get each numerical feature and remove from list\n","    numeric_features = [tf.cast(feat, tf.float32) for feat in numeric_features] #make them float32?\n","    numeric_features = tf.stack(numeric_features, axis=-1) #https://www.tensorflow.org/api_docs/python/tf/stack Basically merges them into one list\n","    features['numeric'] = numeric_features\n","\n","    return features, labels\n","\n","NUMERIC_FEATURES = ['age','n_siblings_spouses','parch', 'fare']\n","\n","packed_train_data = raw_train_data.map( #Remember .map passes the function to each element of the dataset\n","    PackNumericFeatures(NUMERIC_FEATURES))\n","\n","packed_test_data = raw_test_data.map(\n","    PackNumericFeatures(NUMERIC_FEATURES))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QkzHj5Fs8KSE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"5c91819a-0b15-4437-ab0d-447cb4b62384","executionInfo":{"status":"ok","timestamp":1579060874467,"user_tz":360,"elapsed":3172,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["packed_train_data"],"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<MapDataset shapes: (OrderedDict([(sex, (None,)), (class, (None,)), (deck, (None,)), (embark_town, (None,)), (alone, (None,)), (numeric, (None, 4))]), (None,)), types: (OrderedDict([(sex, tf.string), (class, tf.string), (deck, tf.string), (embark_town, tf.string), (alone, tf.string), (numeric, tf.float32)]), tf.int32)>"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"code","metadata":{"id":"kW3_by-m8Nvo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":202},"outputId":"b611a776-e824-4479-c7ea-789d8bb50b12","executionInfo":{"status":"ok","timestamp":1579060874469,"user_tz":360,"elapsed":3164,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["show_batch(packed_train_data) #show batch works becaused packed_train_data is a collection of batches, it shows one of them\n","#It basically made all the numeric data a single 'category' with its 'columns' inside its dimentionality.\n"],"execution_count":53,"outputs":[{"output_type":"stream","text":["sex                 : [b'female' b'male' b'female' b'male' b'male']\n","class               : [b'Third' b'Third' b'Second' b'Second' b'Third']\n","deck                : [b'unknown' b'unknown' b'E' b'unknown' b'unknown']\n","embark_town         : [b'Southampton' b'Cherbourg' b'Southampton' b'Southampton' b'Southampton']\n","alone               : [b'n' b'y' b'y' b'y' b'y']\n","numeric             : [[25.     1.     0.     7.925]\n"," [34.5    0.     0.     6.438]\n"," [27.     0.     0.    10.5  ]\n"," [28.     0.     0.    10.5  ]\n"," [55.5    0.     0.     8.05 ]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QucnhYFr8StD","colab_type":"code","colab":{}},"source":["example_batch, labels_batch = next(iter(packed_train_data)) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QlFPDRzP8g3p","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":351},"outputId":"592f7419-b304-4875-de85-433ec1ab45d0","executionInfo":{"status":"ok","timestamp":1579060874811,"user_tz":360,"elapsed":3491,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["example_batch"],"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OrderedDict([('sex',\n","              <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'male', b'female', b'female', b'female', b'female'], dtype=object)>),\n","             ('class',\n","              <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'Second', b'Third', b'Third', b'Second', b'First'], dtype=object)>),\n","             ('deck',\n","              <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'unknown', b'unknown', b'unknown', b'unknown', b'C'], dtype=object)>),\n","             ('embark_town', <tf.Tensor: shape=(5,), dtype=string, numpy=\n","              array([b'Southampton', b'Queenstown', b'Southampton', b'Southampton',\n","                     b'Southampton'], dtype=object)>),\n","             ('alone',\n","              <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'n', b'y', b'n', b'y', b'n'], dtype=object)>),\n","             ('numeric', <tf.Tensor: shape=(5, 4), dtype=float32, numpy=\n","              array([[ 44.   ,   1.   ,   0.   ,  26.   ],\n","                     [ 21.   ,   0.   ,   0.   ,   7.75 ],\n","                     [  9.   ,   4.   ,   2.   ,  31.275],\n","                     [ 35.   ,   0.   ,   0.   ,  21.   ],\n","                     [ 58.   ,   0.   ,   1.   , 153.462]], dtype=float32)>)])"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"markdown","metadata":{"id":"thgwoOWh8u9_","colab_type":"text"},"source":["**Data Normalization**\n","\n","Continuous data should always be normalized.\n","The continuous data are the numbers..."]},{"cell_type":"code","metadata":{"id":"9XIKUqk18lWJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":300},"outputId":"e65f9b72-78fe-45a6-a4a7-f7e92b442f9c","executionInfo":{"status":"ok","timestamp":1579060874813,"user_tz":360,"elapsed":3484,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["import pandas as pd\n","desc = pd.read_csv(train_file_path)[NUMERIC_FEATURES].describe()\n","desc"],"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>n_siblings_spouses</th>\n","      <th>parch</th>\n","      <th>fare</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>627.000000</td>\n","      <td>627.000000</td>\n","      <td>627.000000</td>\n","      <td>627.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>29.631308</td>\n","      <td>0.545455</td>\n","      <td>0.379585</td>\n","      <td>34.385399</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>12.511818</td>\n","      <td>1.151090</td>\n","      <td>0.792999</td>\n","      <td>54.597730</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.750000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>23.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>7.895800</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>28.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>15.045800</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>35.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>31.387500</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>80.000000</td>\n","      <td>8.000000</td>\n","      <td>5.000000</td>\n","      <td>512.329200</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              age  n_siblings_spouses       parch        fare\n","count  627.000000          627.000000  627.000000  627.000000\n","mean    29.631308            0.545455    0.379585   34.385399\n","std     12.511818            1.151090    0.792999   54.597730\n","min      0.750000            0.000000    0.000000    0.000000\n","25%     23.000000            0.000000    0.000000    7.895800\n","50%     28.000000            0.000000    0.000000   15.045800\n","75%     35.000000            1.000000    0.000000   31.387500\n","max     80.000000            8.000000    5.000000  512.329200"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"u2fPRONGTjVV","colab_type":"code","colab":{}},"source":["#Simple mean and std normalization\n","MEAN = np.array(desc.T['mean']) #desc.T[mean] is the format of numpy to make the mean of each column\n","STD = np.array(desc.T['std'])\n","def normalize_numeric_data(data, mean, std):\n","  # Center the data\n","  return (data-mean)/std\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y5L78iGYTyXp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"8067720b-ff01-48ba-dfbc-f751a55cfe6b","executionInfo":{"status":"ok","timestamp":1579060874815,"user_tz":360,"elapsed":3469,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["#Now create a numeric column. \n","#The tf.feature_columns.numeric_column API accepts a normalizer_fn argument, which will be run on each batch.\n","#This is kinof like a .map function but in numpy and it passes the arguments to every batch and to all data??\n","\n","# See what you just created.\n","normalizer = functools.partial(normalize_numeric_data, mean=MEAN, std=STD)\n","\n","numeric_column = tf.feature_column.numeric_column('numeric', normalizer_fn=normalizer, shape=[len(NUMERIC_FEATURES)])\n","numeric_columns = [numeric_column]\n","numeric_column\n","#I think numeric column gets the mean and std data and uses the normalizer function on every column.\n","#It returns the normalized data as np arrays."],"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["NumericColumn(key='numeric', shape=(4,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function normalize_numeric_data at 0x7ff7ed729620>, mean=array([29.631,  0.545,  0.38 , 34.385]), std=array([12.512,  1.151,  0.793, 54.598])))"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"code","metadata":{"id":"qyd1jgReURuQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":128},"outputId":"58b664dc-af1f-404a-c8a3-7c204696cdce","executionInfo":{"status":"ok","timestamp":1579060874817,"user_tz":360,"elapsed":3457,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["#When you train the model, include this feature column to select and center this block of numeric data:\n","example_batch['numeric']\n"],"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(5, 4), dtype=float32, numpy=\n","array([[ 44.   ,   1.   ,   0.   ,  26.   ],\n","       [ 21.   ,   0.   ,   0.   ,   7.75 ],\n","       [  9.   ,   4.   ,   2.   ,  31.275],\n","       [ 35.   ,   0.   ,   0.   ,  21.   ],\n","       [ 58.   ,   0.   ,   1.   , 153.462]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"code","metadata":{"id":"WHvqf_fSVHNb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":109},"outputId":"0b669de9-925c-461d-8102-8fda91406394","executionInfo":{"status":"ok","timestamp":1579060874818,"user_tz":360,"elapsed":3433,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["#Now we get the data normalized\n","numeric_layer = tf.keras.layers.DenseFeatures(numeric_columns) #Make a layer that normalizes\n","#We are passing the unprocessed data to the model and adding a layer to it that does the processing inside.\n","numeric_layer(example_batch).numpy()\n","\n","#Note: The mean based normalization used here requires knowing the means of each column ahead of time.\n"],"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 1.148,  0.395, -0.479, -0.154],\n","       [-0.69 , -0.474, -0.479, -0.488],\n","       [-1.649,  3.001,  2.043, -0.057],\n","       [ 0.429, -0.474, -0.479, -0.245],\n","       [ 2.267, -0.474,  0.782,  2.181]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"markdown","metadata":{"id":"hQY027tgVsfi","colab_type":"text"},"source":["#Categorical data"]},{"cell_type":"markdown","metadata":{"id":"z4eDNv0hVzdz","colab_type":"text"},"source":["Some of the columns in the CSV data are categorical columns. That is, the content should be one of a limited set of options.\n","\n","Use the tf.feature_column API to create a collection with a tf.feature_column.indicator_column for each categorical column."]},{"cell_type":"code","metadata":{"id":"1J1f8ERGVM7A","colab_type":"code","colab":{}},"source":["#Dictionary? of possible categories\n","CATEGORIES = {\n","    'sex': ['male', 'female'],\n","    'class' : ['First', 'Second', 'Third'],\n","    'deck' : ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'],\n","    'embark_town' : ['Cherbourg', 'Southhampton', 'Queenstown'],\n","    'alone' : ['y', 'n']\n","}\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cAIds328V6wP","colab_type":"code","colab":{}},"source":["#This is to pass the values on the dataset to the ones in the dictionary\n","#Does this count as cleaning?\n","categorical_columns = []\n","for feature, vocab in CATEGORIES.items():\n","  cat_col = tf.feature_column.categorical_column_with_vocabulary_list( #Note all this are legit keras layers https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list\n","        key=feature, vocabulary_list=vocab) #Note: if you hover over the function google colab shows you info of it\n","  categorical_columns.append(tf.feature_column.indicator_column(cat_col))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yBydMtiqWaxV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":129},"outputId":"c8412272-0328-48d3-e8e5-f8be195ce324","executionInfo":{"status":"ok","timestamp":1579060874823,"user_tz":360,"elapsed":3391,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["categorical_columns"],"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='sex', vocabulary_list=('male', 'female'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n"," IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='class', vocabulary_list=('First', 'Second', 'Third'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n"," IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='deck', vocabulary_list=('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n"," IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='embark_town', vocabulary_list=('Cherbourg', 'Southhampton', 'Queenstown'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n"," IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='alone', vocabulary_list=('y', 'n'), dtype=tf.string, default_value=-1, num_oov_buckets=0))]"]},"metadata":{"tags":[]},"execution_count":63}]},{"cell_type":"code","metadata":{"id":"HU0YRLKGWtiy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"97064302-7542-42ae-d5a0-cc51c56da75a","executionInfo":{"status":"ok","timestamp":1579060874825,"user_tz":360,"elapsed":3377,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["categorical_layer = tf.keras.layers.DenseFeatures(categorical_columns)\n","print(categorical_layer(example_batch).numpy()[0])\n","#The 1 and zeroes are the categories converted to a single array.\n"],"execution_count":64,"outputs":[{"output_type":"stream","text":["[0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WG0rE0sOXCp9","colab_type":"text"},"source":["#Combined preprocessing layer\n","\n","Add the two feature column collections and pass them to a tf.keras.layers.DenseFeatures to create an input layer that will extract and preprocess both input types:"]},{"cell_type":"code","metadata":{"id":"FU4ZDf1MW0_e","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"b8dcffa2-9ac3-4fb4-e7dc-aee8cb386c44","executionInfo":{"status":"ok","timestamp":1579060958998,"user_tz":360,"elapsed":406,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["preprocessing_layer = tf.keras.layers.DenseFeatures(categorical_columns+numeric_columns)\n","print(preprocessing_layer(example_batch).numpy()[0])\n","#Basically our input is now this, the normalized continous data and the categories"],"execution_count":68,"outputs":[{"output_type":"stream","text":["[ 0.     1.     0.     1.     0.     0.     0.     0.     0.     0.\n","  0.     0.     0.     0.     0.     0.     0.     0.     1.148  0.395\n"," -0.479 -0.154  1.     0.   ]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FaLCcTLKXhUM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":331},"outputId":"b35ec25c-d5ad-42d5-c9f6-e1541336a4c7","executionInfo":{"status":"ok","timestamp":1579060972380,"user_tz":360,"elapsed":711,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["#the 5 means there are 5 batches.\n","preprocessing_layer(example_batch)"],"execution_count":70,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(5, 24), dtype=float32, numpy=\n","array([[ 0.   ,  1.   ,  0.   ,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n","         0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n","         0.   ,  0.   ,  1.148,  0.395, -0.479, -0.154,  1.   ,  0.   ],\n","       [ 1.   ,  0.   ,  0.   ,  0.   ,  1.   ,  0.   ,  0.   ,  0.   ,\n","         0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n","         0.   ,  1.   , -0.69 , -0.474, -0.479, -0.488,  0.   ,  1.   ],\n","       [ 0.   ,  1.   ,  0.   ,  0.   ,  1.   ,  0.   ,  0.   ,  0.   ,\n","         0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n","         0.   ,  0.   , -1.649,  3.001,  2.043, -0.057,  0.   ,  1.   ],\n","       [ 1.   ,  0.   ,  0.   ,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n","         0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n","         0.   ,  0.   ,  0.429, -0.474, -0.479, -0.245,  0.   ,  1.   ],\n","       [ 0.   ,  1.   ,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ,\n","         0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n","         0.   ,  0.   ,  2.267, -0.474,  0.782,  2.181,  0.   ,  1.   ]],\n","      dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"markdown","metadata":{"id":"PPPN9fFIXaZF","colab_type":"text"},"source":["#Build the model\n","\n","Build a tf.keras.Sequential, starting with the preprocessing_layer."]},{"cell_type":"code","metadata":{"id":"E5AywOJRXGPL","colab_type":"code","colab":{}},"source":["model = tf.keras.Sequential([\n","  preprocessing_layer,\n","  tf.keras.layers.Dense(128, activation='relu'),\n","  tf.keras.layers.Dense(128, activation='relu'),\n","  tf.keras.layers.Dense(1, activation='sigmoid'), #Activation sigmoid: if it survived or not?\n","])\n","\n","model.compile(\n","    loss='binary_crossentropy',\n","    optimizer='adam',\n","    metrics=['accuracy'])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BVE78QbJX8mP","colab_type":"text"},"source":["#Train, evaluate, and predict\n","\n","Now the model can be instantiated and trained."]},{"cell_type":"code","metadata":{"id":"NFfC4Sd2XvzH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":774},"outputId":"f92395fb-a920-45f2-a1d4-04f5283b3f89","executionInfo":{"status":"ok","timestamp":1579061112920,"user_tz":360,"elapsed":9359,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["train_data = packed_train_data.shuffle(500)\n","test_data = packed_test_data\n","model.fit(train_data, epochs=20)\n"],"execution_count":77,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","126/126 [==============================] - 2s 12ms/step - loss: 0.5125 - accuracy: 0.7560\n","Epoch 2/20\n","126/126 [==============================] - 0s 3ms/step - loss: 0.4235 - accuracy: 0.8246\n","Epoch 3/20\n","126/126 [==============================] - 0s 3ms/step - loss: 0.4031 - accuracy: 0.8309\n","Epoch 4/20\n","126/126 [==============================] - 0s 3ms/step - loss: 0.3922 - accuracy: 0.8246\n","Epoch 5/20\n","126/126 [==============================] - 0s 3ms/step - loss: 0.3852 - accuracy: 0.8389\n","Epoch 6/20\n","126/126 [==============================] - 0s 3ms/step - loss: 0.3703 - accuracy: 0.8581\n","Epoch 7/20\n","126/126 [==============================] - 0s 3ms/step - loss: 0.3672 - accuracy: 0.8437\n","Epoch 8/20\n","126/126 [==============================] - 0s 3ms/step - loss: 0.3716 - accuracy: 0.8341\n","Epoch 9/20\n","126/126 [==============================] - 0s 3ms/step - loss: 0.3568 - accuracy: 0.8612\n","Epoch 10/20\n","126/126 [==============================] - 0s 3ms/step - loss: 0.3527 - accuracy: 0.8517\n","Epoch 11/20\n","126/126 [==============================] - 0s 3ms/step - loss: 0.3473 - accuracy: 0.8644\n","Epoch 12/20\n","126/126 [==============================] - 0s 3ms/step - loss: 0.3484 - accuracy: 0.8517\n","Epoch 13/20\n","126/126 [==============================] - 0s 3ms/step - loss: 0.3359 - accuracy: 0.8533\n","Epoch 14/20\n","126/126 [==============================] - 0s 3ms/step - loss: 0.3348 - accuracy: 0.8549\n","Epoch 15/20\n","126/126 [==============================] - 0s 3ms/step - loss: 0.3289 - accuracy: 0.8533\n","Epoch 16/20\n","126/126 [==============================] - 0s 3ms/step - loss: 0.3215 - accuracy: 0.8612\n","Epoch 17/20\n","126/126 [==============================] - 0s 3ms/step - loss: 0.3180 - accuracy: 0.8660\n","Epoch 18/20\n","126/126 [==============================] - 0s 3ms/step - loss: 0.3173 - accuracy: 0.8724\n","Epoch 19/20\n","126/126 [==============================] - 0s 3ms/step - loss: 0.3166 - accuracy: 0.8676\n","Epoch 20/20\n","126/126 [==============================] - 0s 3ms/step - loss: 0.3199 - accuracy: 0.8660\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7ff7ed4f0b70>"]},"metadata":{"tags":[]},"execution_count":77}]},{"cell_type":"code","metadata":{"id":"Dao4gY1hXxjR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":312},"outputId":"3ebf2f7c-fdc9-4b70-b508-0525df2702d2","executionInfo":{"status":"ok","timestamp":1579061117703,"user_tz":360,"elapsed":934,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["#Output shape = multiple? what is that\n","model.summary()"],"execution_count":78,"outputs":[{"output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_features_10 (DenseFeat multiple                  0         \n","_________________________________________________________________\n","dense_9 (Dense)              multiple                  3200      \n","_________________________________________________________________\n","dense_10 (Dense)             multiple                  16512     \n","_________________________________________________________________\n","dense_11 (Dense)             multiple                  129       \n","=================================================================\n","Total params: 19,841\n","Trainable params: 19,841\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Rr19kzwHYH3-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"155e454b-a40b-481f-8243-f49cfa5f9b6d","executionInfo":{"status":"ok","timestamp":1579061187947,"user_tz":360,"elapsed":756,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["test_loss, test_accuracy = model.evaluate(test_data)\n","\n","print('\\n\\nTest Loss {}, Test Accuracy {}'.format(test_loss, test_accuracy))\n"],"execution_count":79,"outputs":[{"output_type":"stream","text":["     53/Unknown - 0s 8ms/step - loss: 0.4333 - accuracy: 0.8523\n","\n","Test Loss 0.43330214608390377, Test Accuracy 0.8522727489471436\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X36n--79YZHA","colab_type":"code","colab":{}},"source":["#Use tf.keras.Model.predict to infer labels on a batch or a dataset of batches.\n","predictions = model.predict(test_data)\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ngt0AqAlYgPp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"8f86e291-9142-4e74-e0bf-a3e073c496b7","executionInfo":{"status":"ok","timestamp":1579061555305,"user_tz":360,"elapsed":694,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["predictions\n","#Literally an array with the value of the sigmoid, it was not a 1 or 0, probably its a probability."],"execution_count":111,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.936],\n","       [0.031],\n","       [0.989],\n","       [0.443],\n","       [0.871],\n","       [0.472],\n","       [0.196],\n","       [0.104],\n","       [0.254],\n","       [0.057],\n","       [0.072],\n","       [0.974],\n","       [0.085],\n","       [0.078],\n","       [0.072],\n","       [0.998],\n","       [0.077],\n","       [0.967],\n","       [0.986],\n","       [0.59 ],\n","       [0.077],\n","       [0.211],\n","       [0.408],\n","       [0.085],\n","       [0.385],\n","       [0.826],\n","       [1.   ],\n","       [0.1  ],\n","       [0.106],\n","       [0.031],\n","       [0.013],\n","       [0.616],\n","       [0.999],\n","       [0.938],\n","       [0.073],\n","       [0.504],\n","       [0.114],\n","       [0.583],\n","       [0.088],\n","       [0.997],\n","       [0.82 ],\n","       [0.115],\n","       [0.083],\n","       [0.042],\n","       [0.088],\n","       [0.128],\n","       [0.734],\n","       [0.073],\n","       [0.867],\n","       [0.999],\n","       [0.078],\n","       [0.411],\n","       [0.066],\n","       [0.394],\n","       [0.45 ],\n","       [0.838],\n","       [0.974],\n","       [0.039],\n","       [0.232],\n","       [0.997],\n","       [0.285],\n","       [0.426],\n","       [0.081],\n","       [0.157],\n","       [0.072],\n","       [0.893],\n","       [0.069],\n","       [0.196],\n","       [0.155],\n","       [0.467],\n","       [0.996],\n","       [0.995],\n","       [0.993],\n","       [0.088],\n","       [0.996],\n","       [0.276],\n","       [0.076],\n","       [0.392],\n","       [0.286],\n","       [0.072],\n","       [0.984],\n","       [0.095],\n","       [0.083],\n","       [0.828],\n","       [0.487],\n","       [0.86 ],\n","       [0.88 ],\n","       [0.202],\n","       [0.195],\n","       [0.131],\n","       [0.039],\n","       [0.072],\n","       [0.909],\n","       [0.023],\n","       [0.196],\n","       [0.196],\n","       [0.019],\n","       [0.006],\n","       [0.075],\n","       [0.074],\n","       [0.758],\n","       [0.091],\n","       [0.055],\n","       [0.236],\n","       [0.003],\n","       [0.074],\n","       [0.854],\n","       [0.118],\n","       [0.976],\n","       [0.008],\n","       [0.979],\n","       [0.165],\n","       [0.083],\n","       [0.27 ],\n","       [0.977],\n","       [0.01 ],\n","       [0.105],\n","       [0.708],\n","       [0.999],\n","       [0.078],\n","       [0.459],\n","       [0.096],\n","       [1.   ],\n","       [0.138],\n","       [0.764],\n","       [0.991],\n","       [0.105],\n","       [0.424],\n","       [0.072],\n","       [0.453],\n","       [0.668],\n","       [0.403],\n","       [1.   ],\n","       [0.977],\n","       [0.064],\n","       [0.01 ],\n","       [0.311],\n","       [0.819],\n","       [0.075],\n","       [0.085],\n","       [0.81 ],\n","       [0.116],\n","       [0.117],\n","       [0.999],\n","       [0.088],\n","       [0.071],\n","       [0.068],\n","       [0.823],\n","       [0.039],\n","       [0.08 ],\n","       [0.055],\n","       [0.697],\n","       [0.076],\n","       [0.815],\n","       [0.086],\n","       [0.995],\n","       [0.991],\n","       [0.018],\n","       [0.039],\n","       [0.078],\n","       [0.983],\n","       [0.071],\n","       [0.467],\n","       [0.097],\n","       [0.056],\n","       [0.087],\n","       [0.462],\n","       [0.491],\n","       [0.072],\n","       [0.227],\n","       [0.824],\n","       [0.251],\n","       [0.854],\n","       [0.196],\n","       [0.826],\n","       [0.999],\n","       [0.072],\n","       [0.062],\n","       [0.099],\n","       [0.076],\n","       [0.08 ],\n","       [0.072],\n","       [0.102],\n","       [0.38 ],\n","       [0.826],\n","       [0.063],\n","       [0.909],\n","       [0.34 ],\n","       [0.458],\n","       [0.012],\n","       [0.957],\n","       [0.066],\n","       [0.909],\n","       [0.869],\n","       [0.957],\n","       [0.106],\n","       [0.091],\n","       [0.825],\n","       [0.088],\n","       [0.826],\n","       [0.827],\n","       [0.345],\n","       [0.012],\n","       [0.747],\n","       [0.081],\n","       [0.066],\n","       [0.091],\n","       [0.239],\n","       [0.946],\n","       [0.435],\n","       [0.073],\n","       [0.056],\n","       [0.037],\n","       [0.38 ],\n","       [0.295],\n","       [0.659],\n","       [0.103],\n","       [0.892],\n","       [0.078],\n","       [0.123],\n","       [0.076],\n","       [0.474],\n","       [0.373],\n","       [0.492],\n","       [0.911],\n","       [0.201],\n","       [0.126],\n","       [0.094],\n","       [0.94 ],\n","       [0.995],\n","       [0.31 ],\n","       [0.092],\n","       [0.258],\n","       [0.996],\n","       [0.246],\n","       [0.116],\n","       [0.015],\n","       [0.096],\n","       [0.168],\n","       [0.097],\n","       [0.467],\n","       [0.067],\n","       [0.425],\n","       [0.02 ],\n","       [0.073],\n","       [0.083],\n","       [0.328],\n","       [0.296],\n","       [0.115],\n","       [0.91 ],\n","       [0.111],\n","       [0.797],\n","       [0.238],\n","       [0.784],\n","       [0.023],\n","       [0.432],\n","       [0.826],\n","       [0.081],\n","       [0.073],\n","       [0.01 ],\n","       [0.978],\n","       [0.064],\n","       [0.77 ],\n","       [0.314]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":111}]},{"cell_type":"code","metadata":{"id":"l7vq1V70Zy0f","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"c246dfb8-de7a-4dd5-de1d-abae6c610066","executionInfo":{"status":"ok","timestamp":1579061556692,"user_tz":360,"elapsed":244,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["#list(test_data) te da una lista con todos los batches.\n","list(test_data)[0][1][:10] #Toma batch 0, array 1 (las labels), #0:10 cambia cada vez que lo corres. creo que estas agarrando para iterar los primeros 10 (como en predictions)\n"],"execution_count":112,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 0, 0, 0, 0], dtype=int32)>"]},"metadata":{"tags":[]},"execution_count":112}]},{"cell_type":"code","metadata":{"id":"hdbpR5cZYgwY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":109},"outputId":"c79c1032-7ac4-4f86-abae-88731297c75a","executionInfo":{"status":"ok","timestamp":1579061627723,"user_tz":360,"elapsed":252,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["# Show some results\n","for prediction, survived in zip(predictions[:10], list(test_data)[0][1][:10]):\n","  print(\"Predicted survival: {:.2%}\".format(prediction[0]),\n","        \" | Actual outcome: \",\n","        (\"SURVIVED\" if bool(survived) else \"DIED\"))\n","#Note it takes different data points every time you run it.\n","#Probably because it takes random batches of data from the possibility of :10"],"execution_count":117,"outputs":[{"output_type":"stream","text":["Predicted survival: 93.64%  | Actual outcome:  SURVIVED\n","Predicted survival: 3.11%  | Actual outcome:  DIED\n","Predicted survival: 98.88%  | Actual outcome:  DIED\n","Predicted survival: 44.26%  | Actual outcome:  DIED\n","Predicted survival: 87.09%  | Actual outcome:  DIED\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BNndxEKWYbYK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"c347abe4-3ff3-4b59-926b-d6ff3ad185c7","executionInfo":{"status":"ok","timestamp":1579061493901,"user_tz":360,"elapsed":213,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":[""],"execution_count":110,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 0, 1, 0, 0], dtype=int32)>"]},"metadata":{"tags":[]},"execution_count":110}]}]}