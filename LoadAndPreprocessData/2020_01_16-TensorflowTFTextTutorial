{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2020_01_16-TensorflowTFTextTutorial","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP6z3wHv1+/5kt6oI+4Ibig"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"czV1pH8__92y","colab_type":"code","colab":{}},"source":["#https://www.tensorflow.org/tutorials/tensorflow_text/intro"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gfsBF6YLAZFO","colab_type":"text"},"source":["#Introduction\n","\n","TensorFlow Text provides a collection of text related classes and ops ready to use with TensorFlow 2.0. The library can perform the preprocessing regularly required by text-based models, and includes other features useful for sequence modeling not provided by core TensorFlow.\n","\n","The benefit of using these ops in your text preprocessing is that they are done in the TensorFlow graph. You do not need to worry about tokenization in training being different than the tokenization at inference, or managing preprocessing scripts."]},{"cell_type":"code","metadata":{"id":"k8RDXLkzANNw","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","# TensorFlow and tf.keras\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jyXJeo3pAVJX","colab_type":"code","colab":{}},"source":["!pip install -q tensorflow-text\n","import tensorflow as tf\n","import tensorflow_text as text"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0SHuNJzjAgp_","colab_type":"text"},"source":["#Unicode\n","\n","Most ops expect that the strings are in UTF-8. If you're using a different encoding, you can use the core tensorflow transcode op to transcode into UTF-8. You can also use the same op to coerce your string to structurally valid UTF-8 if your input could be invalid."]},{"cell_type":"code","metadata":{"id":"ZK_O11B-AfOh","colab_type":"code","colab":{}},"source":["docs = tf.constant([u'Everything not saved will be lost.'.encode('UTF-16-BE'), u'Sad☹'.encode('UTF-16-BE')]) #poner las strings en tensorflow format\n","utf8_docs = tf.strings.unicode_transcode(docs, input_encoding='UTF-16-BE', output_encoding='UTF-8') #pasar de utf-16 a utf-8"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q8VPSnxQBIYr","colab_type":"text"},"source":["#Tokenization\n","\n","Tokenization is the process of breaking up a string into tokens. Commonly, these tokens are words, numbers, and/or punctuation.\n","\n","The main interfaces are `Tokenizer` and `TokenizerWithOffsets` which each have a single method `tokenize` and `tokenize_with_offsets` respectively. There are multiple tokenizers available now. Each of these implement `TokenizerWithOffsets` (which extends `Tokenizer`) which includes an option for getting byte offsets into the original string. This allows the caller to know the bytes in the original string the token was created from.\n","\n","All of the tokenizers return RaggedTensors with the inner-most dimension of tokens mapping to the original individual strings. As a result, the resulting shape's rank is increased by one. Please review the ragged tensor guide if you are unfamiliar with them. https://www.tensorflow.org/guide/ragged_tensors"]},{"cell_type":"markdown","metadata":{"id":"E07jsEP9CQBa","colab_type":"text"},"source":["#WhitespaceTokenizer\n","\n","This is a basic tokenizer that splits UTF-8 strings on ICU defined whitespace characters (eg. space, tab, new line)."]},{"cell_type":"code","metadata":{"id":"GnrZYzlSAtgw","colab_type":"code","outputId":"67a2aad9-d994-4af2-dd16-53cb6e76bce2","executionInfo":{"status":"ok","timestamp":1579212802549,"user_tz":360,"elapsed":4042,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["tokenizer = text.WhitespaceTokenizer()\n","tokens = tokenizer.tokenize(['everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n","print(tokens.to_list())\n"],"execution_count":17,"outputs":[{"output_type":"stream","text":["[[b'everything', b'not', b'saved', b'will', b'be', b'lost.'], [b'Sad\\xe2\\x98\\xb9']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GJilhmObDLJM","colab_type":"text"},"source":["#UnicodeScriptTokenizer\n","\n","This tokenizer splits UTF-8 strings based on Unicode script boundaries. The script codes used correspond to International Components for Unicode (ICU) UScriptCode values. See: http://icu-project.org/apiref/icu4c/uscript_8h.html\n","\n","In practice, this is similar to the WhitespaceTokenizer with the most apparent difference being that it will split punctuation (USCRIPT_COMMON) from language texts (eg. USCRIPT_LATIN, USCRIPT_CYRILLIC, etc) while also separating language texts from each other."]},{"cell_type":"code","metadata":{"id":"cWA97tmQCgxq","colab_type":"code","outputId":"e30b2a05-851a-4753-87d5-61195e9ae0bb","executionInfo":{"status":"ok","timestamp":1579212802552,"user_tz":360,"elapsed":4037,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["tokenizer = text.UnicodeScriptTokenizer()\n","tokens = tokenizer.tokenize(['everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n","print(tokens.to_list())\n"],"execution_count":18,"outputs":[{"output_type":"stream","text":["[[b'everything', b'not', b'saved', b'will', b'be', b'lost', b'.'], [b'Sad', b'\\xe2\\x98\\xb9']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2j_jhWOzDZ5R","colab_type":"text"},"source":["#Unicode split\n","\n","When tokenizing languages without whitespace to segment words (eg. chines?), it is common to just split by character, which can be accomplished using the unicode_split op found in core."]},{"cell_type":"code","metadata":{"id":"rL6xsehjDWQF","colab_type":"code","outputId":"fc8591ec-8f4c-4f26-eac9-bcbda1b9f37c","executionInfo":{"status":"ok","timestamp":1579212802555,"user_tz":360,"elapsed":4033,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["tokens = tf.strings.unicode_split([u\"仅今年前\".encode('UTF-8')], 'UTF-8')\n","print(tokens.to_list())\n"],"execution_count":19,"outputs":[{"output_type":"stream","text":["[[b'\\xe4\\xbb\\x85', b'\\xe4\\xbb\\x8a', b'\\xe5\\xb9\\xb4', b'\\xe5\\x89\\x8d']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2PGtc0jHGHV9","colab_type":"text"},"source":["#Offsets\n","\n","When tokenizing strings, it is often desired to know where in the original string the token originated from. For this reason, each tokenizer which implements `TokenizerWithOffsets` has a `tokenize_with_offsets` method that will return the byte offsets along with the tokens. The offset_starts lists the bytes in the original string each token starts at, and the offset_limits lists the bytes where each token ends."]},{"cell_type":"code","metadata":{"id":"YmbL0yF3Dis1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"176a63d7-acca-4bf7-b49b-b66d3494eaa7","executionInfo":{"status":"ok","timestamp":1579212993626,"user_tz":360,"elapsed":466,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["tokenizer = text.UnicodeScriptTokenizer()\n","(tokens, offset_starts, offset_limits) = tokenizer.tokenize_with_offsets(['everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n","print(tokens.to_list()) #In text\n","print(offset_starts.to_list()) #the integers (including dot)\n","print(offset_limits.to_list()) #The position where each word ends\n"],"execution_count":21,"outputs":[{"output_type":"stream","text":["[[b'everything', b'not', b'saved', b'will', b'be', b'lost', b'.'], [b'Sad', b'\\xe2\\x98\\xb9']]\n","[[0, 11, 15, 21, 26, 29, 33], [0, 3]]\n","[[10, 14, 20, 25, 28, 33, 34], [3, 6]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bTEW-ZpXbhJW","colab_type":"text"},"source":["#TF.Data Example\n","\n","Tokenizers work as expected with the tf.data API. A simple example is provided below."]},{"cell_type":"code","metadata":{"id":"6hIRt9rQbR7v","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":185},"outputId":"d16d998e-7fe4-4736-ef2e-929375c63831","executionInfo":{"status":"ok","timestamp":1579213022296,"user_tz":360,"elapsed":1768,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["docs = tf.data.Dataset.from_tensor_slices([['Never tell me the odds.'], [\"It's a trap!\"]])\n","tokenizer = text.WhitespaceTokenizer()\n","tokenized_docs = docs.map(lambda x: tokenizer.tokenize(x))\n","iterator = iter(tokenized_docs)\n","print(next(iterator).to_list())\n","print(next(iterator).to_list())\n","#A pesar de las warnings, igual te da lo que debe de dar"],"execution_count":24,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f8d3d1ad400> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: No module named 'tensorflow_core.keras'\n","WARNING: AutoGraph could not transform <function <lambda> at 0x7f8d3d1ad400> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: No module named 'tensorflow_core.keras'\n","[[b'Never', b'tell', b'me', b'the', b'odds.']]\n","[[b\"It's\", b'a', b'trap!']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ImryhlSVcB3W","colab_type":"text"},"source":["#Other Text Ops\n","\n","TF.Text packages other useful preprocessing ops. We will review a couple below.\n","Wordshape\n","\n","A common feature used in some natural language understanding models is to see if the text string has a certain property. For example, a sentence breaking model might contain features which check for word capitalization or if a punctuation character is at the end of a string.\n","\n","Wordshape defines a variety of useful regular expression based helper functions for matching various relevant patterns in your input text. Here are a few examples."]},{"cell_type":"code","metadata":{"id":"VopR9nlxbjXK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":91},"outputId":"76e00066-02ad-49d3-f795-d8bd31789498","executionInfo":{"status":"ok","timestamp":1579213370500,"user_tz":360,"elapsed":938,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["#Basicamente con este tipo de procesador de texto puedes separar de varios tipos. \n","\n","tokenizer = text.WhitespaceTokenizer() #A single tokenizer, it does not have text  \n","tokens = tokenizer.tokenize(['Everything not saved will be lost.', u'Sad☹'.encode('UTF-8')]) #pass text to tokenizer\n","\n","# Is capitalized?\n","f1 = text.wordshape(tokens, text.WordShape.HAS_TITLE_CASE)\n","# Are all letters uppercased?\n","f2 = text.wordshape(tokens, text.WordShape.IS_UPPERCASE)\n","# Does the token contain punctuation?\n","f3 = text.wordshape(tokens, text.WordShape.HAS_SOME_PUNCT_OR_SYMBOL)\n","# Is the token a number?\n","f4 = text.wordshape(tokens, text.WordShape.IS_NUMERIC_VALUE)\n","\n","print(f1.to_list())\n","print(f2.to_list())\n","print(f3.to_list())\n","print(f4.to_list())\n","\n"],"execution_count":34,"outputs":[{"output_type":"stream","text":["[[True, False, False, False, False, False], [True]]\n","[[False, False, False, False, False, False], [False]]\n","[[False, False, False, False, False, True], [True]]\n","[[False, False, False, False, False, False], [False]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mSHDAJ7hc8gJ","colab_type":"text"},"source":["#N-grams & Sliding Window\n","\n","N-grams are sequential words given a sliding window size of n. When combining the tokens, there are three reduction mechanisms supported. For text, you would want to use Reduction.STRING_JOIN which appends the strings to each other. The default separator character is a space, but this can be changed with the string_separater argument.\n","\n","The other two reduction methods are most often used with numerical values, and these are Reduction.SUM and Reduction.MEAN."]},{"cell_type":"code","metadata":{"id":"EFdyWMwFcfQ1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"31a65b70-8f4c-4bd6-cee8-bcbc205e0034","executionInfo":{"status":"ok","timestamp":1579213449215,"user_tz":360,"elapsed":458,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["tokenizer = text.WhitespaceTokenizer()\n","tokens = tokenizer.tokenize(['Everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n","\n","# Ngrams, in this case bi-gram (n = 2)\n","bigrams = text.ngrams(tokens, 2, reduction_type=text.Reduction.STRING_JOIN) #Basically join every two words, but inclusive (last word and first of each batch are same)\n","\n","print(bigrams.to_list())\n","\n"],"execution_count":36,"outputs":[{"output_type":"stream","text":["[[b'Everything not', b'not saved', b'saved will', b'will be', b'be lost.'], []]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_A4lWoNcdFx0","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}