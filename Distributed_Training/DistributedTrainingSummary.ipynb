{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DistributedTrainingSummary.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNgURJn0C2+yvloWWJDVJne"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"aW5XhxUidyX4","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cpzVpnbuesHd","colab_type":"text"},"source":["#Data/Object types\n","<ul>\n","\n","  <li>\n","    <a>MirroredStrategy</a>: This object is used to handle distribution when using distributed training with keras\n","  </li>\n","  <li>\n","    <a>OptionsDataset</a>: A dataset obtained from tensorflow mnist:\n","\n","    datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n","    mnist_train, mnist_test = datasets['train'], datasets['test']\n","\n","  </li>\n","  <li>\n","    <a>BatchDataset</a>: When you shuffle and batch an optionsDataset:\n","\n","    train_dataset = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","  </li>\n","  <li>\n","    <a>CachedDataset</a>: When you shuffle, batch and cache an optionsDataset:\n","    \n","    train_dataset = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).cache()\n","  </li>\n","\n","</ul>\n"]},{"cell_type":"markdown","metadata":{"id":"9MHXFoNZe_n3","colab_type":"text"},"source":["#Some useful functions/code snippets:\n","\n","<ul> \n","  <li>\n","    <a>strategy.num_replicas_in_sync</a>: returns the ammount of devices connected for distributed training\n","\n","    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n","\n","  </li>\n","\n","  <li>\n","    <a>model.save </a>, saves the model (including training data) to a path\n","\n","    path = 'saved_model/'\n","    model.save(path, save_format='tf')\n","\n","  Note: To load the model use: <a>tf.keras.models.load_model(path)</a>\n","\n","    #Load the model without strategy.scope.\n","    unreplicated_model = tf.keras.models.load_model(path)\n","\n","    unreplicated_model.compile(\n","        loss='sparse_categorical_crossentropy',\n","        optimizer=tf.keras.optimizers.Adam(),\n","        metrics=['accuracy'])\n","\n","    eval_loss, eval_acc = unreplicated_model.evaluate(eval_dataset)\n","    print('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))\n","  </li>\n","  <li>\n","  <a>tf.data.Dataset.from_tensor_slices((train_data, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE)</a> \n","\n","      Basically transforms the data into a proper tensor to be handled.\n","      It returns a BatchDataset dataType, it also returns shape (None, singe_instance_train_data_shape)\n","  </li>\n","  <li>\n","  <a>model.predict(data_to_predict_from)</a> \n","\n","      returns an array of length (data_to_predict_from) and of the shape you indicated in your model.\n","  </li>\n","\n","</ul>\n"]},{"cell_type":"markdown","metadata":{"id":"zhbwDi1XfYB1","colab_type":"text"},"source":["#Notes\n","<ul>\n","  <li>\n","    When training a model with multiple GPUs, you can use the extra computing power effectively by increasing the batch size. In general, use the largest batch size that fits the GPU memory, and tune the learning rate accordingly.\n","    \n","    #Here we set the batch size as a function of how many GPUs we have\n","    BUFFER_SIZE = 10000\n","    BATCH_SIZE_PER_REPLICA = 64\n","    BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n","  </li>\n","\n","  <li>\n","    An OptionsDataset (and maybe other dataset types) can be shuffled and batched (and chached if memory allows) with this simply snippet:\n","\n","    OptionsDataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).cache()\n","\n","  And it becomes a BatchDataset or a cacheDataset\n","\n","  </li>\n","\n","  <li>\n","    there are different types of model callbacks\n","\n","    1. TensorBoard: This callback writes a log for TensorBoard which allows you to visualize the graphs.\n","    2. Model Checkpoint: This callback saves the model after every epoch.\n","    3. Learning Rate Scheduler: Using this callback, you can schedule the learning rate to change after every epoch/batch.\n","\n","  to define the directory where to store the data:\n","\n","    # Define the checkpoint directory to store the checkpoints\n","    checkpoint_dir = './training_checkpoints'\n","    # Name of the checkpoint files\n","    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","\n","  To add the callbacks:\n","\n","    callbacks = [\n","      tf.keras.callbacks.TensorBoard(log_dir='./logs'), #logs to tensorboard\n","      tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,  #save the state of the model\n","                                        save_weights_only=True),\n","      tf.keras.callbacks.LearningRateScheduler(decay), #change the LR decay. \n","      #decay is a function that takes epoch as argument and changes the LR depending on the epoch\n","      PrintLR()\n","    ]\n","  </li>\n","\n","</ul>"]},{"cell_type":"code","metadata":{"id":"z-fb78V_fN-v","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}