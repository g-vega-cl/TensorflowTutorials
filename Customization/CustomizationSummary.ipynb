{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CustomizationSummary.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNBF5L1Ja+C/uE/mgNXzwy0"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"eTQ2cZ6STFf5","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4NuNPfvjr4qf","colab_type":"text"},"source":["#Common tensorflow program structure:\n","\n","    1. Import and parse the dataset.\n","    2. Select the type of model.\n","    3. Train the model.\n","    4. Evaluate the model's effectiveness.\n","    5. Use the trained model to make predictions.\n"]},{"cell_type":"markdown","metadata":{"id":"GLsdyCMmdhZu","colab_type":"text"},"source":["#Tensors\n","\n","A Tensor is a multi-dimensional array. Similar to NumPy `ndarray` objects, `tf.Tensor` objects have a data type and a shape. Additionally, `tf.Tensors` can reside in accelerator memory (like a GPU). TensorFlow offers a rich library of operations (`tf.add, tf.matmul, tf.linalg.inv `etc.) that consume and produce `tf.Tensors`. These operations automatically convert native Python types, for example:"]},{"cell_type":"markdown","metadata":{"id":"rGlYvYLbtT58","colab_type":"text"},"source":["#Layers: common sets of useful operations\n","\n","Most of the time when writing code for machine learning models you want to operate at a higher level of abstraction than individual operations and manipulation of individual variables.\n","\n","Many machine learning models are expressible as the composition and stacking of relatively simple layers, and TensorFlow provides both a set of many common layers as a well as easy ways for you to write your own application-specific layers either from scratch or as the composition of existing layers.\n","\n","TensorFlow includes the full Keras API in the `tf.keras package`, and the Keras layers are very useful when building your own models.\n","\n","\n","\n","```\n","# In the tf.keras.layers package, layers are objects. To construct a layer,\n","# simply construct the object. Most layers take as a first argument the number\n","# of output dimensions / channels.\n","layer = tf.keras.layers.Dense(100)\n","# The number of input dimensions is often unnecessary, as it can be inferred\n","# the first time the layer is used, but it can be provided if you want to\n","# specify it manually, which is useful in some complex models.\n","layer = tf.keras.layers.Dense(10, input_shape=(None, 5))\n","\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wc7xpbl57_Nk","colab_type":"text"},"source":["#Model\n","A model is a relationship between features and the label.\n","Some simple models can be described with a few lines of algebra, but complex machine learning models have a large number of parameters that are difficult to summarize.\n","\n","Could you determine the relationship between the four features and the Iris species without using machine learning? That is, could you use traditional programming techniques (for example, a lot of conditional statements) to create a model? Perhaps—if you analyzed the dataset long enough to determine the relationships between petal and sepal measurements to a particular species. And this becomes difficult—maybe impossible—on more complicated datasets. A good machine learning approach determines the model for you. If you feed enough representative examples into the right machine learning model type, the program will figure out the relationships for you."]},{"cell_type":"markdown","metadata":{"id":"AzB-QyLz9RTi","colab_type":"text"},"source":["#Create a model using Keras\n","\n","The TensorFlow `tf.keras` API is the preferred way to create models and layers. This makes it easy to build models and experiment while Keras handles the complexity of connecting everything together.\n","\n","The `tf.keras.Sequential` model is a linear stack of layers. Its constructor takes a list of layer instances, and an output layer representing our predictions. The first layer's `input_shape` parameter corresponds to the number of features from the dataset, and is required."]},{"cell_type":"markdown","metadata":{"id":"6ZI52_QTyjjk","colab_type":"text"},"source":["#Steps to build a model \"from scratch\"\n","\n","1. Define the model\n","```\n","    model = tf.keras.Sequential([\n","    tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  # input shape required\n","    tf.keras.layers.Dense(10, activation=tf.nn.relu),\n","    tf.keras.layers.Dense(3)\n","  ])\n","```\n","2. Define the loss function\n","\n","This measures how off a model's predictions are from the desired label, in other words, how bad the model is performing. We want to minimize, or optimize, this value.\n","\n","```\n","#Get the loss (predicted vs real) of the current batch\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","def loss(model, x, y, training):\n","  # training=training is needed only if there are layers with different\n","  # behavior during training versus inference (e.g. Dropout).\n","  y_ = model(x, training=training)\n","\n","  return loss_object(y_true=y, y_pred=y_)\n","```\n","\n","3. Define the gradient function\n","\n","\n","```\n","#Using gradients (derivatives) get the loss_value \n","#(A single value of the 'average error')\n","#And pass it through the trainable_variables of the model\n","#Note, trainable_variables is an array of values of each of  your defined layers.\n","\n","def grad(model, inputs, targets):\n","  with tf.GradientTape() as tape:\n","    loss_value = loss(model, inputs, targets, training=True)\n","  return loss_value, tape.gradient(loss_value, model.trainable_variables)\n","\n","```\n","\n","4. Create an optimizer\n","\n","An optimizer applies the computed gradients to the model's variables to minimize the loss function. \n","```\n","# To push your variables in the right direction\n","#Here we use the SGD optimizer, there are plenty to choose from.\n","#The learning rate is the step size to take for each iteration 'down the hill'\n","optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n","\n","```\n","\n","5. Train the model (Training loop)\n","\n","\n","```\n","# Keep results for plotting\n","train_loss_results = []\n","train_accuracy_results = []\n","\n","num_epochs = 201 #Set number of epochs\n","\n","for epoch in range(num_epochs): #loop per epoch\n","  epoch_loss_avg = tf.keras.metrics.Mean() #get the mean of the loss (accumulative)\n","  epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy() #define the accuracy\n","\n","  # Training loop - using batches of 32\n","  for x, y in train_dataset:\n","    # Optimize the model\n","    loss_value, grads = grad(model, x, y) #get the single loss value and the gradients of each variable\n","    optimizer.apply_gradients(zip(grads, model.trainable_variables)) #apply the gradients to each variable\n","    #the optimizer was defined in previous step\n","\n","    # Track progress\n","    epoch_loss_avg(loss_value)  # Add current batch loss #you do some function (average) to the loss, it keeps memory of previous values \n","    # Compare predicted label to actual label\n","    # training=True is needed only if there are layers with different\n","    # behavior during training versus inference (e.g. Dropout).\n","    epoch_accuracy(y, model(x, training=True)) #This also keeps memory of previous values. \n","\n","  # End epoch\n","  train_loss_results.append(epoch_loss_avg.result())\n","  train_accuracy_results.append(epoch_accuracy.result())\n","\n","  if epoch % 50 == 0:\n","    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n","                                                                epoch_loss_avg.result(),\n","                                                                epoch_accuracy.result()))\n","\n","```\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZhB75zQWgYdT","colab_type":"text"},"source":["#Dataset Types:\n","<ul>\n","  <li>\n","    <a>TensorSliceDataset</a> creates a tensor type from numbers\n","\n","    tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6])\n","\n","  </li>\n","  <li>\n","    <a>TextLineDatasetV2</a> creates a tensor type from text\n","\n","    tf.data.TextLineDataset(filename) \n","  \n","  </li>\n","\n","  <li>\n","    <a> PrefetchDataset </a> is returned from tf.data.experimental.make_csv_dataset\n","\n","    train_dataset = tf.data.experimental.make_csv_dataset(train_dataset_fp,\n","    batch_size, column_names=column_names, label_name=label_name, num_epochs=1)\n","  </li>\n","</ul>"]},{"cell_type":"markdown","metadata":{"id":"xbjYr-hAeRa-","colab_type":"text"},"source":["#Some useful code snippets/functions:\n","\n","<ul>\n","  <li> \n","    <a>tf.config.experimental.list_physical_devices(\"GPU\")</a>\n","    To check if GPU is availiable: \n","\n","    print(\"Is there a GPU available: \"),\n","    print(tf.config.experimental.list_physical_devices(\"GPU\"))\n","    print(\"Is the Tensor on GPU #0:  \"),\n","    print(x.device.endswith('GPU:0')) \n","  </li>\n","  <li> \n","    <a> a_tensor.shuffle(x).batch(x)</a> Te shufflea y batchea el tensor.\n","\n","    *Nota,no se que onda con shuffle (https://www.tensorflow.org/api_docs/python/tf/random/shuffle), \n","    *batch lo que hace es que te los pone en 'batches' del tamaño indicado\n","    *Cabe decir que en batch, si no hay información suficiente para que sea un número perfectamente divisible, \n","    solo te deja el último batch hasta donde tenga info\n","  </li>\n","  <li> \n","    <a>layer.variables</a> y <a>layer.trainable_variables </a> \n","    inspect all variables (or trainable_variables) in a layer\n","\n","      the layer is simply a keras layer, example: \n","      layer = tf.keras.layers.Dense(10, input_shape=(None, 5))\n","      # The variables are also accessible through nice accessors\n","        layer.kernel, layer.bias\n","      \n","  Along the same lines: <a> YourBlock.layers </a> returns the layers of the (possibly custom) model\n","\n","  </li>\n","\n","  <li> <a> tf.data.experimental.make_csv_dataset </a> When the dataset is a CSV function we use this function into a suitable format. Since this function generates data for training models, the default behavior is to shuffle the data (shuffle=True, shuffle_buffer_size=10000), and repeat the dataset forever (num_epochs=None). We also set the batch_size parameter.\n","        \n","    batch_size = 32\n","    train_dataset = tf.data.experimental.make_csv_dataset(train_dataset_fp,\n","    batch_size, column_names=column_names, label_name=label_name, num_epochs=1)\n","\n","  NOTE: this is not enough to start training the data. And usually you have to use a stack function to pass it to <a> MapDataset </a>:\n","\n","    #Define the stack function \n","    def pack_features_vector(features, labels):\n","      \"\"\"Pack the features into a single array.\"\"\"\n","      features = tf.stack(list(features.values()), axis=1)\n","      return features, labels \n","    pass the function to the dataset\n","    train_dataset = train_dataset.map(pack_features_vector)\n","\n","  NOTE #2: for test_datasets you dont have to shuffle it:\n","\n","    test_dataset = tf.data.experimental.make_csv_dataset(test_fp,batch_size,column_names=column_names,label_name=label_name,num_epochs=1,shuffle=False)\n","\n","  </li>\n","\n","  <li><a>tf.convert_to_tensor()</a> This takes a numpy array you give it and makes it a tf.Tensor\n","      \n","    predict_dataset = tf.convert_to_tensor([\n","      [5.1, 3.3, 1.7, 0.5,],\n","      [5.9, 3.0, 4.2, 1.5,],\n","      [6.9, 3.1, 5.4, 2.1]\n","    ])\n","\n","   </li>\n","</ul>"]},{"cell_type":"markdown","metadata":{"id":"txKVe_MUZkLS","colab_type":"text"},"source":["#Gradient\n","https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient\n","\n","The gradient is a way of packing togheter all the partial derivative information of a function."]},{"cell_type":"markdown","metadata":{"id":"fGmdo8ovgXpN","colab_type":"text"},"source":["#Notes:\n","\n","<ul>\n","  <li>\n","    For a custom layer or blockModel, after building it:\n","    \n","    layer = MyCustom(CustomInputs #In layers its usually output_size,\n","    in blocks is the input)\n","    #In this function you get a type <__main__.NameOfCustomClass\n","  It is important to know that after setting up the function, doing the layer = ....\n","  you have to activate it?: like so:\n","  \n","    _ = layer(tf.zeros([10, 5])) #the thing iside is the input\n","\n","  </li>\n","  <li> \n","      Gradients: A list of sum(dy/dx) for each x in xs.\n","      They are an important component of the neural network\n","  </li>\n","\n","  <li> just a reminder: softmax returns odds of every output you give it</li>\n","</ul>"]},{"cell_type":"code","metadata":{"id":"wmVgHFxmdh5U","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}