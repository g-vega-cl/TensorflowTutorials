{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2020_01_23-AutomaticDiferentiationTFTut.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMlzEiomx22A9U9c7MyGYoF"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"AyMakfv2WGrB","colab_type":"code","colab":{}},"source":["#https://www.tensorflow.org/tutorials/customization/autodiff"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K3Oofkh0WLyP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"a777e332-98ff-48a2-ab54-d5955e0b2171","executionInfo":{"status":"ok","timestamp":1579798820202,"user_tz":360,"elapsed":7132,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","# TensorFlow and tf.keras\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np"],"execution_count":2,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fIikCMmiWpXD","colab_type":"text"},"source":["#Gradient tapes\n","\n","TensorFlow provides the `tf.GradientTape` API for automatic differentiation - computing the gradient of a computation with respect to its input variables. Tensorflow \"records\" all operations executed inside the context of a `tf.GradientTape` onto a \"tape\". Tensorflow then uses that tape and the gradients associated with each recorded operation to compute the gradients of a \"recorded\" computation using reverse mode differentiation.\n","\n","For example:"]},{"cell_type":"code","metadata":{"id":"gCbxKGFEWNhS","colab_type":"code","colab":{}},"source":["x = tf.ones((2, 2)) #create a tf matrix of ones shape 2,2\n","\n","with tf.GradientTape() as t:\n","  t.watch(x) #I think this is to record everything that happens to X\n","  y = tf.reduce_sum(x) #Basically sums the insides of the matrix and flattens it into a sole number\n","  z = tf.multiply(y, y) #multiplies that sole number by itself., returns 16 (4*4)\n","\n","# Derivative of z with respect to the original input tensor x\n","dz_dx = t.gradient(z, x) #returns a (2,2) matrix full of 8. Why??, not sure...\n","for i in [0, 1]:\n","  for j in [0, 1]:\n","    assert dz_dx[i][j].numpy() == 8.0 #Note: assert is like an if, if we change it gives an error because the value is not 8.\n","    \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2rnuC75sWwWY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"b10d3166-e0cd-489b-b7ce-2a2285cd9ec1","executionInfo":{"status":"ok","timestamp":1579799311753,"user_tz":360,"elapsed":311,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["dz_dx"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n","array([[8., 8.],\n","       [8., 8.]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"diUNNNaeYItQ","colab_type":"text"},"source":["You can also request gradients of the output with respect to intermediate values computed during a \"recorded\" `tf.GradientTape` context."]},{"cell_type":"code","metadata":{"id":"JA_pSlmJYGoq","colab_type":"code","colab":{}},"source":["x = tf.ones((2, 2))\n","\n","with tf.GradientTape() as t:\n","  t.watch(x)\n","  y = tf.reduce_sum(x)\n","  z = tf.multiply(y, y)\n","\n","# Use the tape to compute the derivative of z with respect to the\n","# intermediate value y.\n","dz_dy = t.gradient(z, y) #returns a single 8\n","assert dz_dy.numpy() == 8.0\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-hdH50ZQYYP-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"5158160c-4110-4ae6-8f4c-0b6f1c9020cf","executionInfo":{"status":"ok","timestamp":1579799415982,"user_tz":360,"elapsed":332,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":["dz_dy"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"hOwWFSSrYg_W","colab_type":"text"},"source":["By default, the resources held by a GradientTape are released as soon as `GradientTape.gradient()` method is called. To compute multiple gradients over the same computation, create a persistent gradient tape. This allows multiple calls to the gradient() method as resources are released when the tape object is garbage collected. For example:"]},{"cell_type":"code","metadata":{"id":"he4yi9kkYgrl","colab_type":"code","colab":{}},"source":["x = tf.constant(3.0) #creates a simple 3\n","with tf.GradientTape(persistent=True) as t:\n","  t.watch(x)\n","  y = x * x #9\n","  z = y * y #81\n","dz_dx = t.gradient(z, x)  # 108.0 (4*x^3 at x = 3) #<- 27*4 = 108, is this the same calc for everything? where does the 4* and the ^3 come from?\n","dy_dx = t.gradient(y, x)  # 6.0 \n","del t  # Drop the reference to the tape\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uSO8gBnbZkWE","colab_type":"text"},"source":["#Recording control flow\n","\n","Because tapes record operations as they are executed, Python control flow (using ifs and whiles for example) is naturally handled:"]},{"cell_type":"code","metadata":{"id":"p2VO06kIYt6V","colab_type":"code","colab":{}},"source":["#f does not return the same value as grad.\n","def f(x, y): # a simple function with for and ifs, Not sure if the values are arbitraty\n","  output = 1.0\n","  for i in range(y):\n","    if i > 1 and i < 5:\n","      output = tf.multiply(output, x)\n","  return output\n","\n","def grad(x, y):\n","  with tf.GradientTape() as t:\n","    t.watch(x)\n","    out = f(x, y)\n","  return t.gradient(out, x)\n","\n","x = tf.convert_to_tensor(2.0) #passing a tensor\n","\n","assert grad(x, 6).numpy() == 12.0\n","assert grad(x, 5).numpy() == 12.0\n","assert grad(x, 4).numpy() == 4.0\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hIBtnVNlast-","colab_type":"text"},"source":["#Higher-order gradients\n","\n","Operations inside of the GradientTape context manager are recorded for automatic differentiation. If gradients are computed in that context, then the gradient computation is recorded as well. As a result, the exact same API works for higher-order gradients as well. For example:"]},{"cell_type":"code","metadata":{"id":"hDArc2XdaeLt","colab_type":"code","colab":{}},"source":["x = tf.Variable(1.0)  # Create a Tensorflow variable initialized to 1.0\n","\n","with tf.GradientTape() as t:\n","  with tf.GradientTape() as t2:\n","    y = x * x * x\n","  # Compute the gradient inside the 't' context manager\n","  # which means the gradient computation is differentiable as well.\n","  dy_dx = t2.gradient(y, x) #returns a 3, whyy????????, i think its because y = x*x*x... because if we do x*x it returns 2...\n","d2y_dx2 = t.gradient(dy_dx, x) #returns a 6....\n","\n","assert dy_dx.numpy() == 3.0\n","assert d2y_dx2.numpy() == 6.0\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iP2e54zxa2YV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"72eb7624-9136-49fb-d7ea-aed3dddf53fc","executionInfo":{"status":"ok","timestamp":1579800068784,"user_tz":360,"elapsed":322,"user":{"displayName":"Cesar vega","photoUrl":"","userId":"04805785551054582010"}}},"source":[""],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=float32, numpy=6.0>"]},"metadata":{"tags":[]},"execution_count":36}]}]}